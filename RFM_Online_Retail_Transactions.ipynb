{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# https://justpaste.it/df554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_excel('Online Retail.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attribute Information**\n",
    "- InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "<br><br>\n",
    "- StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "<br><br>\n",
    "- Description: Product (item) name. Nominal.\n",
    "<br><br>\n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "<br><br>\n",
    "- InvoiceDate: Invoice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "<br><br>\n",
    "- UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "<br><br>\n",
    "- CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "<br><br>\n",
    "- Country: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Findings**\n",
    "- **Potential data reversal**. \n",
    "    - The max and min value of Quantity are both 80995; it could represent a reversal of data entry. However, the issue is that both the initial and reversal entry are retained in the dataset. Further investigation is needed to understand the nature and determine the best way to manage such data reversal.\n",
    "- **Potential indication of cancellation orders from negative UnitPrice**. \n",
    "    - It is uncommon to have negative UnitPrice, as this would mean a cash outflow to a company. These transactions could represent cancelled orders by customers or bad-debt/write-off incurred by the business.\n",
    "- **Missing 25% of CustomerID**. \n",
    "    - The missing unique identified of customers could post a problem as market/customer segmentation requires grouping each unique customer into a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InvoiceDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate Date and Time information from InvoiceDate**\n",
    "- The InvoiceDate column contains both date and time of the transaction. These data are separated into individual columns to facilitate future feature engineering and data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datetime from InvoiceDate\n",
    "df['Date'] = df['InvoiceDate'].dt.date\n",
    "df['Time'] = df['InvoiceDate'].dt.time\n",
    "\n",
    "# Remove InvoiceDate column\n",
    "df.drop(['InvoiceDate'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InvoiceNo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract transaction status from InvoiceNo**\n",
    "- InvoiceNo contains both transaction status (i.e. having a 'C' denotes cancelled transaction) and transaction identifier (e.g. unique invoice number). This information could be extracted to facilitate further feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lambda  x : x**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate order status and invoice number from InvoiceNo\n",
    "df['CancelledOrder'] = df['InvoiceNo'].apply(\n",
    "    lambda x: re.findall(r'[A-Z]', str(x))).apply(lambda x: pd.Series(x))\n",
    "df['Invoice_No'] = df['InvoiceNo'].apply(\n",
    "    lambda x: re.findall(r'\\d+', str(x))).apply(lambda x: pd.Series(x))\n",
    "\n",
    "# Remove old InvoiceNo column\n",
    "df.drop(['InvoiceNo'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode CancelledOrder\n",
    "df['CancelledOrder'] = df['CancelledOrder'].astype('category')\n",
    "df['CancelledOrder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_row(df, column, criterion, operator='equal'):\n",
    "    '''\n",
    "    Filter rows based on specific condition\n",
    "    '''\n",
    "    if operator == 'equal':\n",
    "        return df[df[column] == criterion]\n",
    "    if operator == 'less':\n",
    "        return df[df[column] <= criterion]\n",
    "    if operator == 'more':\n",
    "        return df[df[column] >= criterion]\n",
    "\n",
    "def remove_row(df, column, criterion):\n",
    "    '''\n",
    "    Remove ros based on specific condition\n",
    "    '''\n",
    "    return df[df[column] != criterion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'A' category is unexpected; printing out rows to investigate further\n",
    "filter_row(df, 'CancelledOrder', 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Bad Debt Records**\n",
    "\n",
    "- Bad debt adjustments are dropped from the dataset as these do not represent actual sales. Furthermore, they are not tagged to any specific customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Bad Debt Record\n",
    "df = remove_row(df, 'CancelledOrder', 'A')\n",
    "\n",
    "# Encode column\n",
    "df['CancelledOrder'] = df['CancelledOrder'].cat.add_categories([0])\n",
    "df['CancelledOrder'].fillna(value=0, inplace=True)\n",
    "df['CancelledOrder'].replace(to_replace='C', value=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StockCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StockCode as Categorical Data**\n",
    "- StockCode is a unique identifier assigned to each item and the StockCode should be a category dtype. \n",
    "- Given the number of items, performing One Hot Encoding might not be feasible as this might result in 'curse of dimensionality'. This column will be one-hot encoded at a later stage should the need arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['StockCode'] = df.StockCode.astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total spending of customers could be derived for additional datapoint**\n",
    "- The unit price represents the price of a single item; a new column ('TotalSum') could be created to represent the total price paid by the customer for the respective purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalSum'] = df['Quantity'] * df['UnitPrice']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rows with 0 TotalSum\n",
    "filter_row(df, 'TotalSum', 0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing rows with 0 TotalSum**\n",
    "- **(DISCUSSION)** Rows with 0 TotalSum seems to serve as recording for misc activities; further discussion with business analysts is needed to understand the nature of such data. Pending such, these rows are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_row(df, 'TotalSum', 0)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View rows with 0 TotalSum\n",
    "filter_row(df, 'TotalSum', 0, 'less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any TotalSum with less than 0 do not belong to cancelled order\n",
    "df[df['TotalSum'] <= 0][df['CancelledOrder'] == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative TotalSum is due to negative quantity**\n",
    "\n",
    "- Negative TotalSum is caused by negative quantity. Further investigation will be made later for such negative quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print description list\n",
    "print(df['Description'].unique())\n",
    "\n",
    "print('\\n Number of unique items: {}'.format(df['Description'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description could harbour irrelevant data**\n",
    "- Description describes the items/activities. The list of items could potentially other non-relevant data for customer segmentation. Such irrelevant item/activities will be removed at second iteration of data-preprocessing or feature engineering, if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with negative quantity\n",
    "filter_row(df, 'Quantity', 0, 'less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative quantity denotes cancelled transaction**\n",
    "- It appears that negative quantity represents the cancelled transaction. However, the exact reason for the cancellation is not known.\n",
    "<br><br>\n",
    "- There are two ways to manage the cancelled orders:\n",
    "    - Account for both corresponding initial orders and cancelled orders to remove sales from the dataset, since there is no actual profit made from the cancelled orders\n",
    "    - Drop only the cancelled orders to maximise customers' data, though it will also capture the wrong purchase made by customers.\n",
    "<br><br>\n",
    "- **(DISCUSSION)** The first approach will reflect the actual purchase made, as cancelled orders could represent wrong order by customers and this doesn't reflect the actual customer's purchase intent. The value of corresponding purchases and the cancelled transaction will be aggregated and hence the rows value will offset each other naturally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with missing customerID\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**25% of critical information missing from dataset**<br>\n",
    "There is a total of 132,602 rows (~25% of total data) with missing information on CustomerID. CustomerID holds the identity of the customer and without which, it will be impossible to perform customer segmentation.\n",
    "\n",
    "**Exploring Data Imputation Based on InvoiceNo**<br>\n",
    "The missing values could be imputed based on other features such as InvoiceNo since the same customer would probably buy for the items under the same invoice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows with missing CustomerID\n",
    "df[df['CustomerID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows with missing CustomerID with CancelledOrder\n",
    "df[df['CustomerID'].isnull() & df['CancelledOrder'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context**\n",
    "- **(ASSUMPTION)** Missing CustomerID will be imputed with random numbers that are not existing in the dataset. Since this is an online purchase, these transactions could be made under 'Guest Checkout' which doesn't require the customer to create an account.\n",
    "- **(ASSUMPTION)** These transactions could likely be a one-time purchase as customers who shop frequently would probably create an account for ease of purchasing and perhaps earn rewards.\n",
    "\n",
    "**Rows with missing customerID will be dropped**\n",
    "- **(DISCUSSION)** As discovered earlier, the corresponding purchase and cancelled transaction do not have same InvoiceNo. Though it is possible to impute CustomerID based on the unique value of InvoiceNo, there will be big inaccuracy in matching cancelled transactions. This could result in issues when the values are summed up to offset each other at a later stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['CustomerID'])]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting country location**\n",
    "- The long and lat location of respective country could be added to represent the geographical distance between customers (customers residing in countries near each other might have similar purchase behaviour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import country long/lat location from google\n",
    "country_loc = pd.read_html(\n",
    "    'https://developers.google.com/public-data/docs/canonical/countries_csv', header=0)\n",
    "\n",
    "# Convert data to df\n",
    "country_loc_df = pd.DataFrame(data=country_loc[0])\n",
    "\n",
    "# Drop country abbreviation\n",
    "country_loc_df.drop(['country'], axis=1, inplace=True)\n",
    "\n",
    "# Rename index column\n",
    "country_loc_df.rename(index=str, columns={'name':'Country'}, inplace=True)\n",
    "\n",
    "sorted(country_loc_df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to store result of whether all countries in df are present in country_loc_df\n",
    "df['Exist'] = df['Country'].isin(country_loc_df['Country'])\n",
    "\n",
    "# Print out countires which are not present in df\n",
    "df[df['Exist'] == False]['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling missing and mismatched country name**\n",
    "- A quick search of Google shows that EIRE is Irish for Ireland and RSA for South Africa. These will be replaced accordingly.\n",
    "- The USA will be replaced with United States; European Community will be replaced with France; the Channel Islands will be replaced with Jersey\n",
    "- There are 224 rows without country information (Unspecified). This could be problematic to assign the long and lat information. These countries rows will be imputed with the most frequent countries: United Kingdom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace countries\n",
    "df.loc[df['Country'] == 'EIRE', 'Country'] = 'Ireland'\n",
    "df.loc[df['Country'] == 'RSA', 'Country'] = 'South Africa'\n",
    "df.loc[df['Country'] == 'USA', 'Country'] = 'United States'\n",
    "df.loc[df['Country'] == 'European Community', 'Country'] = 'France'\n",
    "df.loc[df['Country'] == 'Channel Islands', 'Country'] = 'Jersey'\n",
    "df.loc[df['Country'] == 'Unspecified', 'Country'] = 'United Kingdom'\n",
    "\n",
    "# Drop Exist column created\n",
    "df = df.drop(columns=['Exist'])\n",
    "\n",
    "# Combine data together\n",
    "df = df.merge(country_loc_df, on=['Country', 'Country'])\n",
    "\n",
    "# Convert country to categorical dtype\n",
    "df['Country'] = df.Country.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Time Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    '''\n",
    "    Prase year and month values\n",
    "\n",
    "    '''\n",
    "    return dt.datetime(x.year, x.month, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get month information from dataframe\n",
    "df['InvoiceMonth'] = df['Date'].apply(get_date)\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each customer based on their ID and select InvoiceMonth value\n",
    "grouping = df.groupby('CustomerID')['InvoiceMonth']\n",
    "\n",
    "# Get first month of each customer acqusition\n",
    "df['CohortMonth'] = grouping.transform('min')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_int(dataframe, column):\n",
    "    '''\n",
    "    Extract year, month, and day values\n",
    "\n",
    "    '''\n",
    "    year = dataframe[column].dt.year\n",
    "    month = dataframe[column].dt.month\n",
    "    day = dataframe[column].dt.day\n",
    "    return year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, and day from InvoiceMonth and CohortMonth\n",
    "invoice_year, invoice_month, invoice_day = get_date_int(df, 'InvoiceMonth')\n",
    "cohort_year, cohort_month, cohort_day = get_date_int(df, 'CohortMonth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between first acqusition and date of each transaction\n",
    "years_diff = invoice_year - cohort_year\n",
    "months_diff = invoice_month - cohort_month\n",
    "days_diff = invoice_day - cohort_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time offset in months\n",
    "df['CohortIndex'] = (years_diff * 12 + months_diff + 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count monthly active customers from each cohort\n",
    "grouping = df.groupby(['CohortMonth', 'CohortIndex'])\n",
    "\n",
    "# Count the number of customers in each group via nunique of customerID\n",
    "cohort_data = grouping['CustomerID'].apply(pd.Series.nunique)\n",
    "\n",
    "# Reset index\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "cohort_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for customer retention\n",
    "cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "cohort_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_avg_value(cohort_grouping, column, decimal=3):\n",
    "\n",
    "    # Count average value in each cohort\n",
    "    cohort_data = grouping[column].mean()\n",
    "\n",
    "    # Reset index of df\n",
    "    cohort_data = cohort_data.reset_index()\n",
    "\n",
    "    # Create a pivot table of average customer purchase value\n",
    "    average_quantity = cohort_data.pivot(\n",
    "        index='CohortMonth', columns='CohortIndex', values=column)\n",
    "\n",
    "    # Round figures\n",
    "    average_quantity = average_quantity.round(decimal)\n",
    "\n",
    "    return average_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store first column as cohort size\n",
    "cohort_sizes = cohort_counts.iloc[:, 0]\n",
    "\n",
    "# Calculate percentage of active customers in each cohort\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
    "\n",
    "# Convert retention to percentage format\n",
    "retention = retention.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average unit price per cohort\n",
    "avg_cohort_price = cohort_avg_value(grouping, 'UnitPrice', 2)\n",
    "\n",
    "# Calculate the avg quantity of items purchased in each cohort\n",
    "avg_cohort_quantity = cohort_avg_value(grouping, 'Quantity', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 3 graphs together for summary findings\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Retention Rate Per Cohort')\n",
    "ax = sns.heatmap(retention, annot=True, fmt='.0%', vmin=0, vmax=0.5)\n",
    "ax.set_yticklabels(retention.index.date)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Average Quanity Per Cohort')\n",
    "ax = sns.heatmap(avg_cohort_quantity, annot=True)\n",
    "ax.set_yticklabels(avg_cohort_quantity.index.date)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Average Price Per Cohort')\n",
    "ax = sns.heatmap(avg_cohort_price, annot=True)\n",
    "ax.set_yticklabels(avg_cohort_price.index.date)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customer retention rate drops over time**\n",
    "- Customer retention metric measures 2 things:\n",
    "    1. How many customers are originally in each cohort (acquisition month)\n",
    "    2. How many of them are still active in the following months\n",
    "<br><br>\n",
    "- Approximately 25% of new customers acquired will make a repeat purchase, and this percentage remains fairly consistently barring occasionally increase and dips in activities.\n",
    "\n",
    "\n",
    "**Quantity of items purchased seems to increase during holiday period**\n",
    "- The number of items purchased remain fairly constant and increased sharply during the holiday period.\n",
    "\n",
    "**Bigger purchases are made during holiday period**\n",
    "- Customers tend to make a bigger purchase during the holiday period; this could be due to a bigger discount/incentive given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recency, Frequency, and Monetary (RFM) Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFM is a behavioural customer segmentation for measuring customer value.\n",
    "- Receny: How recent was each customer's last purchase\n",
    "- Frequency: How many times the customer purchased in the last 12 months\n",
    "- Monetary Value: How much has the customer spent in the last 12 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating RFM Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 1-year date range from latest data\n",
    "earliest_date = df['Date'].min()\n",
    "end_date = df['Date'].max()\n",
    "\n",
    "print(\"Actual Start Date: {}, Actual End Date: {}\".format(earliest_date, end_date))\n",
    "\n",
    "# Filter 1-year data range from original df\n",
    "start_date = end_date - pd.to_timedelta(364, unit='d')\n",
    "df_rfm = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "\n",
    "print(\"RFM Start Date: {}, RFM End Date: {}\".format(\n",
    "    df_rfm['Date'].min(), df_rfm['Date'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypothetical snapshot date\n",
    "snapshot_date = end_date + dt.timedelta(days=1)\n",
    "\n",
    "# Calculate Recency, Frequency and Monetary value for each customer\n",
    "df_rfm = df_rfm.groupby(['CustomerID']).agg({\n",
    "    'Date': lambda x: (snapshot_date - x.max()).days,\n",
    "    'Invoice_No': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "# Rename the columns\n",
    "df_rfm.rename(columns={'Date': 'Recency',\n",
    "                       'Invoice_No': 'Frequency',\n",
    "                       'TotalSum': 'MonetaryValue'}, inplace=True)\n",
    "\n",
    "# Print top 5 rows\n",
    "print(df_rfm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RFM Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning quantile to each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain quantile of each customer\n",
    "def get_quantile(df, column, start_n_quantiles, end_n_quantiles, step=1):\n",
    "\n",
    "    category_label = range(start_n_quantiles, end_n_quantiles, step)\n",
    "    \n",
    "    quantiles = pd.qcut(df[column], q=abs(\n",
    "        end_n_quantiles - start_n_quantiles), labels=category_label)\n",
    "\n",
    "    df = df.assign(name=quantiles.values)\n",
    "\n",
    "    new_column_name = column + '_Q'\n",
    "\n",
    "    return df.rename(columns={\"name\": new_column_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_quantile = df_rfm.copy()\n",
    "\n",
    "df_rfm_quantile = get_quantile(df_rfm_quantile, 'Recency', 4, 0, -1)\n",
    "df_rfm_quantile = get_quantile(df_rfm_quantile, 'Frequency', 1, 5)\n",
    "df_rfm_quantile = get_quantile(df_rfm_quantile, 'MonetaryValue', 1, 5)\n",
    "\n",
    "df_rfm_quantile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RFM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate RFM quartile values\n",
    "def join_rfm(x): \n",
    "    return str(x['Recency_Q']) + str(x['Frequency_Q']) + str(x['MonetaryValue_Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form RFM segment\n",
    "df_rfm_quantile['RFM_Segment'] = df_rfm_quantile.apply(join_rfm, axis=1)\n",
    "\n",
    "df_rfm_quantile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM_Score\n",
    "df_rfm_quantile['RFM_Score'] = df_rfm_quantile[['Recency_Q','Frequency_Q','MonetaryValue_Q']].sum(axis=1)\n",
    "\n",
    "df_rfm_quantile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing RFM Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_quantile.groupby('RFM_Segment').size().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_quantile[df_rfm_quantile['RFM_Segment']=='111'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Metrics per RFM score\n",
    "df_rfm_quantile.groupby('RFM_Score').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count'] }).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_me(df):\n",
    "    if df['RFM_Score'] >= 9:\n",
    "        return '1.Gold'\n",
    "    elif (df['RFM_Score'] >= 5) and (df['RFM_Score'] < 9):\n",
    "        return '2.Silver'\n",
    "    else:\n",
    "        return '3.Bronze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_quantile['General_Segment'] = df_rfm_quantile.apply(segment_me, axis=1)\n",
    "\n",
    "df_rfm_custom_segment = df_rfm_quantile.groupby('General_Segment').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "df_rfm_custom_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means assume symmertical distribution of varibles and variables have equal average value and std.\n",
    "- If the distribution is not symmetrical\n",
    "    - Log transformation (if all values are positive)\n",
    "    - Add the absolute value of the lowest negative value to each observation, and then with a small constant (e.g. 1) to force all variables to be positive\n",
    "    - Use a cube root transformation\n",
    "- If the mean and variables are not equal, the variables could be standardised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skew(df, column):\n",
    "    skew = stats.skew(df[column])\n",
    "    skewtest = stats.skewtest(df[column])\n",
    "    plt.title('Distribution of ' + column)\n",
    "    sns.distplot(df[column])\n",
    "    print(\"{}'s: Skew: {}, : {}\".format(column, skew, skewtest))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 3 graphs together for summary findings\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "check_skew(df_rfm,'Recency')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "check_skew(df_rfm,'Frequency')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "check_skew(df_rfm,'MonetaryValue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('before_transform.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original to new df\n",
    "df_rfm_log = df_rfm.copy()\n",
    "\n",
    "# Data Pre-Processing for Negative Value\n",
    "df_rfm_log['MonetaryValue'] = (df_rfm_log['MonetaryValue'] - df_rfm_log['MonetaryValue'].min()) + 1\n",
    "\n",
    "df_rfm_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform skewed data with log tranasformation\n",
    "df_rfm_log = np.log(df_rfm_log)\n",
    "\n",
    "# Check for skewness after log transformation\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "check_skew(df_rfm_log,'Recency')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "check_skew(df_rfm_log,'Frequency')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "check_skew(df_rfm_log,'MonetaryValue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('after_transform.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_rfm_log)\n",
    "df_rfm_normal = scaler.transform(df_rfm_log)\n",
    "\n",
    "df_rfm_normal = pd.DataFrame(df_rfm_normal, index=df_rfm_log.index, columns=df_rfm_log.columns)\n",
    "\n",
    "# Check result after standardising\n",
    "df_rfm_normal.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two primary methods to define number of clusters:\n",
    "- Elbow criterion (visual method)\n",
    "    - Plot number of clusters against within-cluster sum-of-squared-errors (SSE) - sum of squared distances from every data point to their cluster cente\n",
    "<br><br>\n",
    "- Silhouette Score (math method)\n",
    "    - Measures intra- and inter-cluster distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_kmeans(dataset, start=2, end=11):\n",
    "    '''\n",
    "    Calculate the optimal number of kmeans\n",
    "    \n",
    "    INPUT:\n",
    "        dataset : dataframe. Dataset for k-means to fit\n",
    "        start : int. Starting range of kmeans to test\n",
    "        end : int. Ending range of kmeans to test\n",
    "    OUTPUT:\n",
    "        Values of Inertia and Silhouette Score.\n",
    "        Lineplot of values of Inertia and Silhouette Score.\n",
    "    '''\n",
    "    \n",
    "    # Create empty lists to store values for plotting graphs\n",
    "    n_clu = []\n",
    "    km_ss = []\n",
    "    inertia = []\n",
    "\n",
    "    # Create a for loop to find optimal n_clusters\n",
    "    for n_clusters in range(start, end):\n",
    "\n",
    "        # Create cluster labels\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(dataset)\n",
    "\n",
    "        # Calcualte model performance\n",
    "        silhouette_avg = round(silhouette_score(dataset, labels, random_state=1), 3)\n",
    "        inertia_score = round(kmeans.inertia_, 2)\n",
    "\n",
    "        # Append score to lists\n",
    "        km_ss.append(silhouette_avg)\n",
    "        n_clu.append(n_clusters)\n",
    "        inertia.append(inertia_score)\n",
    "\n",
    "        print(\"No. Clusters: {}, Silhouette Score(SS): {}, SS Delta: {}, Inertia: {}, Inertia Delta: {}\".format(\n",
    "            n_clusters, \n",
    "            silhouette_avg, \n",
    "            (km_ss[n_clusters - start] - km_ss[n_clusters - start - 1]).round(3), \n",
    "            inertia_score, \n",
    "            (inertia[n_clusters - start] - inertia[n_clusters - start - 1]).round(3)))\n",
    "\n",
    "        # Plot graph at the end of loop\n",
    "        if n_clusters == end - 1:\n",
    "            plt.figure(figsize=(9,6))\n",
    "\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.title('Within-Cluster Sum-of-Squares / Inertia')\n",
    "            sns.pointplot(x=n_clu, y=inertia)\n",
    "\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.title('Silhouette Score')\n",
    "            sns.pointplot(x=n_clu, y=km_ss)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_kmeans(df_rfm_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the inertia and silhouette score, the optimal number of cluster is 4. However, during the implemention of KMEans, cluster of 3, 4, and 5 will be tested to experiment which cluster makes most business sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(normalised_df_rfm, clusters_number, original_df_rfm):\n",
    "    '''\n",
    "    Implement k-means clustering on dataset\n",
    "    \n",
    "    INPUT:\n",
    "        normalised_df_rfm : dataframe. Normalised rfm dataset for k-means to fit.\n",
    "        clusters_number : int. Number of clusters to form.\n",
    "        original_df_rfm : dataframe. Original rfm dataset to assign the labels to.\n",
    "    OUTPUT:\n",
    "        Cluster results and t-SNE visualisation of clusters.\n",
    "    '''\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = clusters_number, random_state = 1)\n",
    "    kmeans.fit(normalised_df_rfm)\n",
    "\n",
    "    # Extract cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "        \n",
    "    # Create a cluster label column in original dataset\n",
    "    df_new = original_df_rfm.assign(Cluster = cluster_labels)\n",
    "    \n",
    "    # Initialise TSNE\n",
    "    model = TSNE(random_state=1)\n",
    "    transformed = model.fit_transform(df_new)\n",
    "    \n",
    "    # Plot t-SNE\n",
    "    plt.title('Flattened Graph of {} Clusters'.format(clusters_number))\n",
    "    sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"Set1\")\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "df_rfm_k3 = kmeans(df_rfm_normal, 3, df_rfm)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "df_rfm_k4 = kmeans(df_rfm_normal, 4, df_rfm)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "df_rfm_k5 = kmeans(df_rfm_normal, 5, df_rfm)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('flattened.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Customer Personas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_values(df):\n",
    "    '''\n",
    "    Calcualte average RFM values and size for each cluster\n",
    "\n",
    "    '''\n",
    "    df_new = df.groupby(['Cluster']).agg({\n",
    "        'Recency': 'mean',\n",
    "        'Frequency': 'mean',\n",
    "        'MonetaryValue': ['mean', 'count']\n",
    "    }).round(0)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_values(df_rfm_k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_values(df_rfm_k4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_values(df_rfm_k5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_plot(normalised_df_rfm, df_rfm_kmeans, df_rfm_original):\n",
    "    '''\n",
    "    Transform dataframe and plot snakeplot\n",
    "    '''\n",
    "    # Transform df_normal as df and add cluster column\n",
    "    normalised_df_rfm = pd.DataFrame(normalised_df_rfm, \n",
    "                                       index=df_rfm.index, \n",
    "                                       columns=df_rfm.columns)\n",
    "    normalised_df_rfm['Cluster'] = df_rfm_kmeans['Cluster']\n",
    "\n",
    "    # Melt data into long format\n",
    "    df_melt = pd.melt(normalised_df_rfm.reset_index(), \n",
    "                        id_vars=['CustomerID', 'Cluster'],\n",
    "                        value_vars=['Recency', 'Frequency', 'MonetaryValue'], \n",
    "                        var_name='Metric', \n",
    "                        value_name='Value')\n",
    "\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Value')\n",
    "    sns.pointplot(data=df_melt, x='Metric', y='Value', hue='Cluster')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Snake Plot of K-Means = 3')\n",
    "snake_plot(df_rfm_normal, df_rfm_k3, df_rfm)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Snake Plot of K-Means = 4')\n",
    "snake_plot(df_rfm_normal, df_rfm_k4, df_rfm)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Snake Plot of K-Means = 5')\n",
    "snake_plot(df_rfm_normal, df_rfm_k5, df_rfm)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Importance of Segment Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_importance(df_rfm_kmeans, df_rfm_original):\n",
    "    '''\n",
    "    Calculate relative importance of segment attributes and plot heatmap\n",
    "    '''\n",
    "    # Calculate average RFM values for each cluster\n",
    "    cluster_avg = df_rfm_kmeans.groupby(['Cluster']).mean() \n",
    "\n",
    "    # Calculate average RFM values for the total customer population\n",
    "    population_avg = df_rfm.mean()\n",
    "\n",
    "    # Calculate relative importance of cluster's attribute value compared to population\n",
    "    relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "    sns.heatmap(data=relative_imp, annot=True, fmt='.2f')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Relative Importance of K-Means = 3')\n",
    "relative_importance(df_rfm_k3, df_rfm)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Relative Importance of K-Means = 4')\n",
    "relative_importance(df_rfm_k4, df_rfm)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Relative Importance of K-Means = 5')\n",
    "relative_importance(df_rfm_k5, df_rfm)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal K-Means Clustering (For Medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_kmeans(dataset, start=2, end=11):\n",
    "    '''\n",
    "    Calculate the optimal number of kmeans\n",
    "    \n",
    "    INPUT:\n",
    "        dataset : dataframe. Dataset for k-means to fit\n",
    "        start : int. Starting range of kmeans to test\n",
    "        end : int. Ending range of kmeans to test\n",
    "    OUTPUT:\n",
    "        Values and line plot of Silhouette Score.\n",
    "    '''\n",
    "    \n",
    "    # Create empty lists to store values for plotting graphs\n",
    "    n_clu = []\n",
    "    km_ss = []\n",
    "\n",
    "    # Create a for loop to find optimal n_clusters\n",
    "    for n_clusters in range(start, end):\n",
    "\n",
    "        # Create cluster labels\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(dataset)\n",
    "\n",
    "        # Calcualte model performance\n",
    "        silhouette_avg = round(silhouette_score(dataset, labels, random_state=1), 3)\n",
    "\n",
    "        # Append score to lists\n",
    "        km_ss.append(silhouette_avg)\n",
    "        n_clu.append(n_clusters)\n",
    "\n",
    "        print(\"No. Clusters: {}, Silhouette Score: {}, Change from Previous Cluster: {}\".format(\n",
    "            n_clusters, \n",
    "            silhouette_avg, \n",
    "            (km_ss[n_clusters - start] - km_ss[n_clusters - start - 1]).round(3)))\n",
    "\n",
    "        # Plot graph at the end of loop\n",
    "        if n_clusters == end - 1:\n",
    "            plt.figure(figsize=(6.47,4))\n",
    "\n",
    "            plt.title('Silhouette Score of Different Number of Clusters')\n",
    "            plt.ylabel('Silhouette Score')\n",
    "            plt.xlabel('Number of Clusters')\n",
    "            sns.pointplot(x=n_clu, y=km_ss)\n",
    "            plt.savefig('silhouette_score.png', format='png', dpi=1000)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
